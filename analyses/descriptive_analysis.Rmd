---
title: "Descriptive Statistics"
---

```{r include = FALSE}
knitr::opts_chunk$set(
  echo = TRUE, 
  warning = FALSE,
  message = FALSE,
  error = FALSE, 
  cache = FALSE
)
```

## Data import

```{r}
library(dplyr)
library(ggplot2)
library(lubridate)

theme_set(theme_classic())

codes_path <- file.path('..', 'data', 'codes.csv')
reddit_path <- file.path('..', 'data', 'reddit.csv')
quotes_path <- file.path('..', 'data', 'quotes.csv')
docs_path <- file.path('..', 'data', 'docs.csv')
urls_path <- file.path('..', 'data', 'urls.csv')

codes_df <- readr::read_csv(codes_path, show_col_types = FALSE)
quotes_df <- readr::read_csv(quotes_path, show_col_types = FALSE)
docs_df <- readr::read_csv(docs_path, show_col_types = FALSE)
urls_df <- readr::read_csv(urls_path, show_col_types = FALSE)

# for some reason the datetime column reads as UTC, not US central so I manually set it here
reddit_df <- 
  readr::read_csv(reddit_path, show_col_types = FALSE) %>% 
  mutate(created_utc = lubridate::as_datetime(created_utc, tz='US/Central'))

```

## Number of comments

How many comments in the megathread according to Reddit API?

What limitations on comments that this does not include (in API docs)?

```{r}
nrow(reddit_df)
```

How many comments were imported into Atlas for coding? 

```{r}
nrow(docs_df)
```


How many comments were deleted by their author (text reads: "[REMOVED]")?

```{r}
nrow(filter(docs_df, removed == TRUE))
```

Leaving us with total comments that could have been coded:

```{r}
docs_df <- filter(docs_df, removed == FALSE)

nrow(docs_df)
```

The number of comments that did get coded?

```{r}
nrow(docs_df %>% filter(is.na(codes)))
```

## Number of redditors

Unique redditors in the thread:

```{r}
reddit_df %>%
  filter(!is.na(author)) %>%
  select(author) %>%
  unique() %>%
  nrow()
```

Comments per author:

```{r}
comments_per_author <-
  reddit_df %>%
  filter(!is.na(author)) %>%
  count(author, name = 'n_comments')

comments_per_author
```

```{r}
comments_per_author %>%
  count(n_comments, name = 'frequency')
```

### Activity level

```{r}
case_activity_level <- function(n_comments) {
  case_when(
    n_comments == 1 ~ 'low',
    n_comments <= 5 ~ 'med',
    n_comments >= 6 ~ 'high'
  )
}

comments_per_author <- 
  comments_per_author %>%
  mutate('activity_level' = case_activity_level(n_comments))

comments_per_author %>%
  group_by(activity_level) %>%
  summarize(
    accounts = n(),
    comments = sum(n_comments)
  )
```


```{r}
comments_per_author %>%
  count(n_comments, name = 'frequency') %>%
  ggplot(aes(x = n_comments, y = frequency)) +
  geom_bar(stat = 'identity', width = 0.5, fill = 'tomato2') +
  xlim(c(0, 25)) +
  labs(title = 'Posts per author distribution') +
  xlab(label = 'n Posts') +
  ylab(label = 'Frequency') 

```

## Time ranges

What is the time range of the thread?

### First comment

```{r}
slice_min(reddit_df, created_utc)['created_utc']
```

### Last comment


```{r}
slice_max(reddit_df, created_utc)['created_utc']
```

### Comments per day

Looking at comments per day

```{r}
reddit_df %>%
  filter(!is.na(author)) %>%
  mutate(daily = floor_date(created_utc, 'days')) %>%
  count(daily) 
```


```{r}
reddit_df %>%
  filter(!is.na(author)) %>%
  mutate(daily = ceiling_date(created_utc, 'days')) %>%
  count(daily) %>%
  ggplot(aes(x = daily, y = n)) +
  geom_bar(stat = 'identity')
```

### Comments per hour

What was the peak period in comments per hour?

```{r}
reddit_df %>%
  filter(!is.na(author)) %>%
  mutate(hourly = floor_date(created_utc, 'hours')) %>%
  count(hourly) %>%
  arrange(hourly)

```

I would say peak period was January 6th, 3:00 pm (15:00) through January 10th, 12:00 pm (23:59)

```{r peak_period}
peak_period_start <- '2021-01-06 15:00:00'
peak_period_end <- '2021-01-11 00:00:00'
```

Peak period close-up:


```{r }
p <-
  reddit_df %>%
  filter(!is.na(author)) %>%
  filter(created_utc > '2021-01-06 15:00:00' & created_utc < '2021-01-11 00:00:00') %>%
  mutate(hourly = floor_date(created_utc, 'hours')) %>%
  count(hourly) %>%
  ggplot(aes(x = hourly, y = n)) +
  #geom_line(aes(y = n)) +
  geom_bar(stat = 'identity') +
  labs(title = 'Comment rate during peak period',
       subtitle = 'Comments per hour between January 6th and January 10th') +
  xlab(label = '') +
  ylab(label = 'comments per hour') 

p

ggsave('comment_rate_peak.png', plot = last_plot(), scale = 1, dpi = 300)

```

## Codes 

Total number of codes assigned


```{r}
nrow(codes_df)
```

Average codes per document

```{r}
codes_df %>%
  count(doc_id) %>%
  summarize(mean = mean(n))
```

Code frequencies

```{r}
codes_df %>%
  count(code) %>%
  mutate(code = forcats::fct_reorder(code, n, .desc = FALSE)) %>%
  ggplot(aes(x = code, y = n)) +
  geom_bar(stat = 'identity', width = 0.5, fill = 'tomato2') +
  coord_flip() +
  labs(title = 'Code frequencies',
       subtitle = 'All codes') +
  xlab(label = '') +
  ylab(label = '')
```
       
```{r}
non_activity_codes <- c(
  '!!!', 
  'r/datahorders', 
  'outside_project', 
  'scope', 
  'participants', 
  'fbi'
)


p <- codes_df %>%
  distinct(doc_id, code) %>%
  filter(!(code %in% non_activity_codes)) %>%
  count(code) %>%
  mutate(code = forcats::fct_reorder(code, n, .desc = FALSE)) %>%
  ggplot(aes(x = code, y = n)) +
  geom_bar(stat = 'identity', width = 0.5, fill = 'tomato2') +
  coord_flip() +
  labs(title = 'Thread activities') +
  xlab(label = '') +
  ylab(label = 'Number of comments')

p

ggsave('thread_activities.png', plot = last_plot(), scale = 1, dpi = 300)

```

Code stats

```{r}
p <- reddit_df %>%
  select(doc_id = comment_id, author, score) %>%
  left_join(codes_df) %>%
  left_join(comments_per_author) %>%
  filter(!is.na(code)) %>%
  filter(!is.na(activity_level)) %>%
  filter(!(code %in% non_activity_codes)) %>%
  group_by(code) %>%
  ggplot(aes(x = code, fill = activity_level)) +
  geom_bar(stat = 'count', 
           position = position_dodge(),
           width = 0.7) +
  scale_fill_manual(values = c('low' = 'firebrick', 'med' = 'steelblue', 'high' = 'darkgreen')) +
  xlab('') +
  labs(title = 'Thread activities by author engagement level') +
  coord_flip()

p

ggsave('activities_by_engagement_level.png', plot = last_plot(), scale = 1, dpi = 300)

```


## Productive / Unproductive comments

"productive" vs. "unproductive" comments 

Grouping codes into categories

```{r}
code_categories <- tribble(
  ~code,             ~category,
  '!!!',             'unrelated',
  'r/datahorders',   'unrelated',
  'outside_project', 'unrelated',
  'scope',           'unrelated',
  'participants',    'unrelated',
  'fbi',             'unrelated',
  'evidence',				 'active_work',
  'saving',	         'active_work',
  'strategy',	       'active_work',
  'seeding',         'active_work',
  'analysis',				 'commentary',
  'encouragement',   'commentary',
  'caution',				 'commentary',
  'motivation',			 'commentary',
  'removal',         'commentary',
  'doxing',				   'unproductive',
  'arguing',				 'unproductive',
  'NA',              'unproductive'
)
```





```{r}
codes_df %>%
  mutate(code = tidyr::replace_na(code, 'NA')) %>%
  left_join(code_categories) %>%
  left_join(reddit_df, by = c('doc_id' = 'comment_id')) %>%
  filter(category != 'unrelated') %>%
  filter(created_utc > '2021-01-06 15:00:00' & created_utc < '2021-01-11 00:00:00') %>%
  mutate(hourly = floor_date(created_utc, 'hours')) %>%
  group_by(hourly, category) %>%
  summarize(category_comments_per_hour = n()) %>%
  unique() %>%
  tidyr::pivot_wider(names_from = category, values_from = category_comments_per_hour, values_fill = 0) %>%
  ggplot(aes(x = hourly)) +
  geom_area(aes(y = active_work + commentary + unproductive, fill = 'active_work')) +
  geom_area(aes(y = commentary + unproductive, fill = 'commentary')) +
  geom_area(aes(y = unproductive, fill = 'unproductive')) +
  labs(title = 'Comment rate by activity type',
       subtitle = 'During peak activity, Jan. 6th - Jan. 9th') +
  xlab(label = '') +
  ylab(label = 'comments per hour') +
  scale_fill_manual(name='',
                    values = c('active_work' = '#00ba38', 'commentary' = '#f8766d', 'unproductive' = '#8bf4f4')) +  # line color
  theme(panel.grid.minor = element_blank())  # turn off minor grid

```

# URL analysis

## Duplicate URLS

Upvotes for duplicate URL submissions:

```{r}
# need to account for a document containing duplicate URLs in itself

duplicate_submissions <- 
  urls_df %>%
  filter(!is.na(content)) %>%
  distinct(document_id, content, .keep_all = TRUE) %>%
  group_by(content) %>%
  mutate(n_submitted = n()) %>%
  ungroup() %>%
  filter(n_submitted > 1)  %>%
  select(document_id, profile, content, n_submitted)

```

How many submissions total? 

```{r}
nrow(urls_df)
```


How many comparable submissions? (coming from a major source, able to parse out account name, content name from URL string)

```{r}
nrow(filter(urls_df, !is.na(content)))
```

How many unique comparable submissions?

```{r}
nrow(filter(urls_df, !is.na(content)) %>% distinct(content))
```

How many submissions duplicated a previously submitted URL?

```{r}
nrow(filter(urls_df, !is.na(content))) - nrow(filter(urls_df, !is.na(content)) %>% distinct(content))
```

Comparing upvotes for duplicated submissions

```{r}
p <- 
  duplicate_submissions %>%
  left_join(reddit_df, by = c('document_id' = 'comment_id')) %>%
  select(content, n_submitted, created_utc, score, author, document_id) %>%
  arrange(desc(n_submitted), content, created_utc) %>%
  group_by(content) %>%
  mutate(order = dense_rank(created_utc)) %>%
  mutate(group_max = max(score)) %>%
  ungroup() %>%
  mutate(is_max = if_else(score == group_max, TRUE, FALSE)) %>%
  group_by(content) %>%
  filter(max(order) > 2) %>%
  filter(max(score) - min(score) > 5) %>%
  mutate(content = group_indices()) %>%
  ggplot(aes(x = order, y = score)) +
  geom_line(size = 0.5) +
  geom_point(aes(color = is_max), size = 0.6) +
  theme(axis.ticks = element_blank(),
        axis.text.x = element_blank(),
        legend.position = 'none') +
  labs(title = 'Score for duplicate URL submissions') +
  xlab(label = '') +
  ylab(label = 'score') +
  scale_y_log10() +
  scale_color_manual(values = c('black', 'red')) +
  facet_wrap(~ content, scales = 'fixed', nrow = 5, shrink = FALSE)

p

ggsave('duplicate_url_scores.png', plot = last_plot(), scale = 1, dpi = 300)
```


## Motivation


```{r}
quotes_df %>%
  left_join(codes_df) %>%
  filter(code == 'motivation') %>%
  select(doc_id, quote) %>%
  readr::write_csv(., file = file.path('motivations.csv'))
```





